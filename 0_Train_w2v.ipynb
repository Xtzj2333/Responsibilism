{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0fdb1b3",
   "metadata": {},
   "source": [
    "### Download Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d65a52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from convokit import Corpus, download, get_subreddit_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0832bb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "uppercase_states = [\n",
    "    'Alabama', 'Alaska', 'Arkansas', 'Arizona', 'California', 'Colorado', 'Connecticut', 'Delaware', 'Florida', 'Georgia',\n",
    "    'Hawaii', 'Idaho', 'Illinois', 'Indiana', 'Iowa', 'Kansas', 'Kentucky', 'Louisiana',\n",
    "    'Maine', 'Maryland', 'Massachusetts', 'Michigan', 'Minnesota', 'Mississippi', 'Missouri',\n",
    "    'Montana', 'Nebraska', 'Nevada', 'New Hampshire', 'New Jersey', 'New Mexico', 'New York',\n",
    "    'North Carolina', 'North Dakota', 'Ohio', 'Oklahoma', 'Oregon', 'Pennsylvania', 'Rhode Island',\n",
    "    'South Carolina', 'South Dakota', 'Tennessee', 'Texas', 'Utah', 'Vermont', 'Virginia',\n",
    "    'Washington', 'West Virginia', 'Wisconsin', 'Wyoming'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a743c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lowercase_states = [state.lower() for state in uppercase_states]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb6805b",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = {}\n",
    "\n",
    "for index, state in enumerate(lowercase_states):\n",
    "    concatenated_state = state.replace(\" \", \"\")\n",
    "    camel_case_state = ''.join(word.capitalize() for word in state.split())\n",
    "    state_variants = [state, concatenated_state, camel_case_state]\n",
    "    loaded = False \n",
    "\n",
    "    print(state_variants)\n",
    "\n",
    "    for variant in state_variants:\n",
    "        corpus_name = f'subreddit-{variant}'\n",
    "        try:\n",
    "            file = download(corpus_name)\n",
    "            print(f\"VARIANT USED: '{variant}'\")\n",
    "            loaded = True\n",
    "            break\n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "    if not loaded:\n",
    "        print(f\"Failed to load corpus for {state} in all variants: last tried '{variant}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc10dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "save_directory = '/data/corpus'\n",
    "\n",
    "os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "for state, corpus_obj in corpus.items():\n",
    "    state_save_path = os.path.join(save_directory, state)\n",
    "\n",
    "    corpus_obj.dump(name=state, base_path=save_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3c8896",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f8d359",
   "metadata": {},
   "outputs": [],
   "source": [
    "from convokit import Corpus\n",
    "import os\n",
    "\n",
    "save_directory = '/models'\n",
    "\n",
    "os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "corpus_load = {}\n",
    "\n",
    "for state_dir in os.listdir(save_directory):\n",
    "    state_path = os.path.join(save_directory, state_dir)\n",
    "\n",
    "    loaded_corpus = Corpus(filename=state_path)\n",
    "    corpus_load[state_dir] = loaded_corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3209911f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import spacy\n",
    "from gensim.models import Word2Vec\n",
    "import multiprocessing\n",
    "import os\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20f126e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_utterances(corpus):\n",
    "    return [utt.text for utt in corpus.iter_utterances()]\n",
    "\n",
    "def tokenize_and_remove_stopwords(text):\n",
    "    if text.strip() in [\"[deleted]\", \"[removed]\"] or not text.strip():\n",
    "        return []\n",
    "    doc = nlp(text)\n",
    "    filtered_tokens = [token.text.lower() for token in doc if token.is_alpha and not token.is_stop and not token.is_punct and not token.like_url]\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8297f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dir = '/data/tokenized_corpus'\n",
    "os.makedirs(tokenized_dir, exist_ok=True)\n",
    "\n",
    "for state, state_corpus in corpus.items():\n",
    "\n",
    "    tokenized_path = os.path.join(tokenized_dir, f\"{state}_tokenized.txt\")\n",
    "\n",
    "    if os.path.exists(tokenized_path):\n",
    "        print(f\"Tokenized corpus for {state} already exists. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Processing corpus for {state}...\")\n",
    "    utts = all_utterances(state_corpus)\n",
    "    tokenized_utts = [tokenize_and_remove_stopwords(utt) for utt in utts]\n",
    "\n",
    "    tokenized_utts = [tokens for tokens in tokenized_utts if tokens]\n",
    "\n",
    "    with open(tokenized_path, 'w') as f:\n",
    "        for tokens in tokenized_utts:\n",
    "            f.write(\" \".join(tokens) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cfa827",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import multiprocessing\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "word2vec_dir = 'models'\n",
    "os.makedirs(word2vec_dir, exist_ok=True)\n",
    "\n",
    "def read_tokenized_data(tokenized_dir, state):\n",
    "    tokenized_path = os.path.join(tokenized_dir, f\"{state}_tokenized.txt\")\n",
    "    with open(tokenized_path, 'r') as f:\n",
    "        tokenized_data = [line.split() for line in f]\n",
    "    return tokenized_data\n",
    "\n",
    "\n",
    "state_data = {}\n",
    "\n",
    "for state in corpus.keys():\n",
    "    print(f\"Training Word2Vec model for {state}...\")\n",
    "\n",
    "    tokenized_utts = read_tokenized_data(tokenized_dir, state)\n",
    "\n",
    "    model = Word2Vec(\n",
    "        tokenized_utts,\n",
    "        vector_size=100,\n",
    "        window=8,\n",
    "        min_count=5,\n",
    "        workers=multiprocessing.cpu_count()\n",
    "    )\n",
    "    state_data[state] = {\n",
    "        'state_name': state,\n",
    "        'word2vec_model': model\n",
    "    }\n",
    "\n",
    "    model_path = os.path.join(word2vec_dir, f\"{state}.kv\")\n",
    "    model.wv.save(model_path)\n",
    "\n",
    "for state, data in state_data.items():\n",
    "    print(f\"State: {state}, Model Info: {data['word2vec_model']}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
